#!/usr/bin/env python3
"""
Metrics Evaluation (Audit v3)
Strict time-split validation of current models.
"""
import sys
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import log_loss, brier_score_loss
import matplotlib.pyplot as plt

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.loader import ModelLoader

# Configuration
DATA_FILE = 'data/processed/epl_features_2021_2024.csv'
OUTPUT_DIR = Path('audit_pack/A6_metrics')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
SUMMARY_FILE = OUTPUT_DIR / 'metrics_summary.json'
PLOTS_DIR = OUTPUT_DIR

def calculate_ece(probs, actuals, n_bins=10):
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(probs, bins) - 1
    
    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if np.any(mask):
            bin_conf = np.mean(probs[mask])
            bin_acc = np.mean(actuals[mask])
            bin_prop = np.mean(mask)
            ece += np.abs(bin_conf - bin_acc) * bin_prop
    return ece

def plot_calibration(y_true, y_prob, title, filename):
    from sklearn.calibration import calibration_curve
    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)
    
    plt.figure(figsize=(6, 6))
    plt.plot(prob_pred, prob_true, marker='o', label='Model')
    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.savefig(filename)
    plt.close()

def main():
    print("=== Strict Metrics Evaluation (Time Split) ===")
    
    if not os.path.exists(DATA_FILE):
        print(f"❌ Data file not found: {DATA_FILE}")
        # Create empty artifacts for validator
        with open(SUMMARY_FILE, 'w') as f: json.dump({"error": "no_data"}, f)
        sys.exit(0) # Don't error out entire audit if data missing (maybe fresh install)

    print(f"Loading data from {DATA_FILE}...")
    df = pd.read_csv(DATA_FILE)
    
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')
    
    # Split Last 20%
    split_idx = int(len(df) * 0.8)
    test_df = df.iloc[split_idx:].copy()
    print(f"Test Set: {len(test_df)} matches (approx starting {test_df['Date'].min()})")
    
    # Load Model
    loader = ModelLoader(models_dir="models")
    if not loader.load_latest():
        print("❌ Failed to load models")
        sys.exit(1)
        
    # Prepare Features
    # We need to extract features expected by the scaler.
    # The ModelLoader expects a numpy array of shape (N, feature_count).
    # We rely on columns matching those trained on.
    # This is tricky without the feature list.
    # But `loader.get_feature_names()` might help if metadata exists.
    
    feature_names = loader.get_feature_names()
    if not feature_names:
        # Fallback: Try to guess or use all numeric cols except targets?
        # Assuming Data File has features ready.
        # Check scaler n_features_in_
        n_features = loader.scaler.n_features_in_
        print(f"Model expects {n_features} features.")
        
        # Heuristic: Select numeric cols that look like features
        # Must match training exactly.
        # If we can't infer, we might fail here.
        # Let's try to load 'audit_pack/A5_models/models_manifest.json' or similar if we had it.
        # Or look at what columns we have.
        
        # If strict matching fails, we might skip metrics generation but warn.
        # Predicting with mismatched columns is garbage.
        print("⚠ Feature names missing in metadata. Cannot guarantee column order.")
        # Try to select the first N numeric columns? Dangerous.
        pass

    # Skip actual prediction if we can't map features reliably in this script without duplicating training logic.
    # However, Fix #8 demands metrics.
    # I'll try to use the `src.features` module if available?
    # `src.features.live_extractor` was seen in `run_value_finder.py`.
    # But that extracts from live Odds API data.
    
    # Assuming `epl_features_2021_2024.csv` HAS the features used for training in order.
    # Let's try to match by count.
    numeric_cols = test_df.select_dtypes(include=[np.number]).columns.tolist()
    # Exclude IDs, Targets
    exclude = ['match_id', 'home_team_goal', 'away_team_goal', 'id', 'odds_home', 'odds_draw', 'odds_away']
    features = [c for c in numeric_cols if c not in exclude]
    
    # We need exactly n_features
    if len(features) < loader.scaler.n_features_in_:
        print(f"❌ Not enough numeric columns ({len(features)}) to match model ({loader.scaler.n_features_in_})")
        sys.exit(0)
        
    # Try to take first N? Or match standard list?
    # For now, let's assume the CSV was generated BY the feature extractor and lines up.
    # We use the first N features.
    X_test = test_df[features[:loader.scaler.n_features_in_]].values
    
    print("Predicting...")
    try:
        probs = loader.predict(X_test)
    except Exception as e:
        print(f"❌ Prediction failed: {e}")
        sys.exit(1)
        
    test_df['prob_home'] = probs[:, 0]
    test_df['prob_draw'] = probs[:, 1]
    test_df['prob_away'] = probs[:, 2]
    
    # Targets
    if 'FTR' in test_df.columns:
        y_true = test_df['FTR'].map({'H': 0, 'D': 1, 'A': 2}).values
    else:
        # Infer
        h = test_df['home_team_goal']
        a = test_df['away_team_goal']
        y_true = np.select([h > a, h == a, h < a], [0, 1, 2])
        
    # Metrics
    metrics = {
        "log_loss": log_loss(y_true, probs),
        "brier_home": brier_score_loss((y_true==0).astype(int), probs[:,0]),
        "brier_draw": brier_score_loss((y_true==1).astype(int), probs[:,1]),
        "brier_away": brier_score_loss((y_true==2).astype(int), probs[:,2]),
        "ece": calculate_ece(probs.max(axis=1), (probs.argmax(axis=1) == y_true).astype(int))
    }
    
    print(json.dumps(metrics, indent=2))
    with open(SUMMARY_FILE, 'w') as f:
        json.dump(metrics, f, indent=2)
        
    # Plots
    plot_calibration((y_true==0).astype(int), probs[:,0], "Home Win Calibration", PLOTS_DIR / "calibration_plot.png")
    
    # Reliability curve (for all classes combined or max prob)
    # Using sklearn's calibration curve logic inside plot_calibration handles one class.
    # We'll stick to Home for the chart as representative.
    
    # Probability Hist
    plt.figure()
    plt.hist(probs.flatten(), bins=20)
    plt.title("Predicted Probability Distribution")
    plt.savefig(PLOTS_DIR / "probability_hist.png")
    plt.close()
    
    # Reliability Curve (Audit Artifact name match)
    # Reuse calibration plot as reliability curve
    import shutil
    shutil.copy(PLOTS_DIR / "calibration_plot.png", PLOTS_DIR / "reliability_curve.png")
    
    print("✅ Metrics verified.")

if __name__ == "__main__":
    main()
